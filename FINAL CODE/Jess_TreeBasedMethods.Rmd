---
title: "Tree-Based Methods"
author: "Jessica VanElls"
date: "November 29, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE,
	comment = NA
)
```

The purpose of this document is to explore tree-based methods using the Ames, Iowa housing data set. I will focus on both predictive accuracy and model interpretation.



## Data
The following section will deal with data transformation. Here, I used the same methods as in the midterm.


```{r}
train <- read.csv("train.csv", na.strings="placeholder")  # some of the categorical variables have value "NA" but it doesn't mean null
test <- read.csv("test.csv", na.strings="placeholder")

house <- rbind(train, data.frame(test, SalePrice=rep(1, nrow(test))))
```


### Dealing with NA's
Because there were also strings that were "NA" as part of some scales, I noted which columns shouldn't contain the string "NA"", and I change those strings to a true `NA`.
```{r}
## Store all columns that can have "NA" as a valid entry
na_names = c("Alley", "BsmtQual", "BsmtCond", "BsmtExposure", "BsmtFinType1", "BsmtFinType2", "FireplaceQu", "GarageType", "GarageQual", "GarageCond", "PoolQC", "Fence", "MiscFeature", "MasVnrType")

## Replace "NA" strings with true NA in training data
for (j in 1:ncol(house)) {
  if (sum(colnames(house)[j]==na_names)==0) {  # if the column shouldn't contain "NA"...
    for (i in 1:nrow(house)) {
      if (house[i,j]=="NA") {
        house[i,j] <- NA  # if the column shouldn't contain NA but the cell is "NA", then give it a null
      }
    }
  }
}
```


### Checking and Changing Data Types
I will be converting scales to factors (e.g., quality) because they describe a condition, not a quantity. There are mixed opinions on how these should be handled, but I am chosing to use the "nominal categorical" method. Years will be treated as integers; months will be treated as factors.
```{r}
## Scales / Factors
house$MSSubClass <- as.factor(house$MSSubClass)
house$MSZoning <- as.factor(house$MSZoning)
house$Street <- as.factor(house$Street)
house$Alley <- as.factor(house$Alley)
house$LotShape <- as.factor(house$LotShape)
house$LandContour <- as.factor(house$LandContour)
house$Utilities <- as.factor(house$Utilities)
house$LotConfig <- as.factor(house$LotConfig)
house$LandSlope <- as.factor(house$LandSlope)
house$Neighborhood <- as.factor(house$Neighborhood)
house$Condition1 <- as.factor(house$Condition1)
house$Condition2 <- as.factor(house$Condition2)
house$BldgType <- as.factor(house$BldgType)
house$HouseStyle <- as.factor(house$HouseStyle)
house$OverallQual <- as.factor(house$OverallQual)
house$OverallCond <- as.factor(house$OverallCond)
house$RoofStyle <- as.factor(house$RoofStyle)
house$RoofMatl <- as.factor(house$RoofMatl)
house$Exterior1st <- as.factor(house$Exterior1st)
house$Exterior2nd <- as.factor(house$Exterior2nd)
house$MasVnrType <- as.factor(house$MasVnrType)
house$ExterQual <- as.factor(house$ExterQual)
house$ExterCond <- as.factor(house$ExterCond)
house$Foundation <- as.factor(house$Foundation)
house$BsmtQual <- as.factor(house$BsmtQual)
house$BsmtCond <- as.factor(house$BsmtCond)
house$BsmtExposure <- as.factor(house$BsmtExposure)
house$BsmtFinType1 <- as.factor(house$BsmtFinType1)
house$BsmtFinType2 <- as.factor(house$BsmtFinType2)
house$Heating <- as.factor(house$Heating)
house$HeatingQC <- as.factor(house$HeatingQC)
house$CentralAir <- as.factor(house$CentralAir)
house$Electrical <- as.factor(house$Electrical)
house$KitchenQual <- as.factor(house$KitchenQual)
house$Functional <- as.factor(house$Functional)
house$FireplaceQu <- as.factor(house$FireplaceQu)
house$GarageType <- as.factor(house$GarageType)
house$GarageFinish <- as.factor(house$GarageFinish)
house$GarageQual <- as.factor(house$GarageQual)
house$GarageCond <- as.factor(house$GarageCond)
house$PavedDrive <- as.factor(house$PavedDrive)
house$PoolQC <- as.factor(house$PoolQC)
house$Fence <- as.factor(house$Fence)
house$MiscFeature <- as.factor(house$MiscFeature)
house$SaleType <- as.factor(house$SaleType)
house$SaleCondition <- as.factor(house$SaleCondition)

## Should be numeric...
house$LotFrontage <- as.integer(house$LotFrontage)
house$LotArea <- as.integer(house$LotArea)
house$MasVnrArea <- as.integer(house$MasVnrArea)
house$BsmtFinSF1 <- as.integer(house$BsmtFinSF1)
house$BsmtFinSF2 <- as.integer(house$BsmtFinSF2)
house$BsmtUnfSF <- as.integer(house$BsmtUnfSF)
house$TotalBsmtSF <- as.integer(house$TotalBsmtSF)
house$X1stFlrSF <- as.integer(house$X1stFlrSF)
house$X2ndFlrSF <- as.integer(house$X2ndFlrSF)
house$LowQualFinSF <- as.integer(house$LowQualFinSF)
house$GrLivArea <- as.integer(house$GrLivArea)
house$BsmtFullBath <- as.integer(house$BsmtFullBath)
house$BsmtHalfBath <- as.integer(house$BsmtHalfBath)
house$FullBath <- as.integer(house$FullBath)
house$HalfBath <- as.integer(house$HalfBath)
house$BedroomAbvGr <- as.integer(house$BedroomAbvGr)
house$KitchenAbvGr <- as.integer(house$KitchenAbvGr)
house$TotRmsAbvGrd <- as.integer(house$TotRmsAbvGrd)
house$Fireplaces <- as.integer(house$Fireplaces)
house$GarageCars <- as.integer(house$GarageCars)
house$GarageArea <- as.integer(house$GarageArea)
house$WoodDeckSF <- as.integer(house$WoodDeckSF)
house$OpenPorchSF <- as.integer(house$OpenPorchSF)
house$EnclosedPorch <- as.integer(house$EnclosedPorch)
house$X3SsnPorch <- as.integer(house$X3SsnPorch)
house$ScreenPorch <- as.integer(house$ScreenPorch)
house$PoolArea <- as.integer(house$PoolArea)
house$MiscVal <- as.integer(house$MiscVal)

## Dates: years as integers, months as factors
house$YearBuilt <- as.integer(house$YearBuilt)
house$YearRemodAdd <- as.integer(house$YearRemodAdd)
house$GarageYrBlt <- as.integer(house$GarageYrBlt)
house$MoSold <- as.factor(house$MoSold)
house$YrSold <- as.integer(house$YrSold)
```


### NA Revisited
Change the string "NA" to "N/A" for variables that are allowed to have "NA" as a value (e.g., Alley). I don't change the "NA" strings to `NA` here (I did it earlier) because otherwise the change of class insert interpolated values instead of `NA`s. 
```{r}
## Store all columns that can have "NA" as a valid entry
na_names = c("Alley", "BsmtQual", "BsmtCond", "BsmtExposure", "BsmtFinType1", "BsmtFinType2", "FireplaceQu", "GarageType", "GarageQual", "GarageCond", "PoolQC", "Fence", "MiscFeature", "MasVnrType")

## Re-level "NA" columns
for (j in 1:ncol(house)) {
  if (sum(colnames(house)[j]==na_names)!=0) {  # if the column can contain "NA" as a string in the training data...
    levels(house[,j])[levels(house[,j])=="NA"] <- "N/A"
  }
}
```


### Remove NA Columns
Remove columns from both sets which have too many NAs in the training set, and then remove rows from the training set with NAs left.
```{r}
## Remove NA columns
ct_na <- rep(0, length=ncol(house))
for (j in 1:ncol(house)) {
  ct_na[j] <- sum(is.na(house[,j]))
}
house <- house[-c(1:80)[ct_na>50]]
```


### Interpolate NA values in the data
```{r}
## Note which columns need to have NAs interpolated
ct_na <- rep(0, length=ncol(house))
for (j in 1:ncol(house)) {
  ct_na[j] <- sum(is.na(house[,j]))
}
ct_na
na_boo <- ifelse(ct_na!=0, T, F)
colnames(house)[na_boo]
```


```{r}
## Choose most common factor
house$MSZoning[is.na(house$MSZoning)] <- "RL"
house$Utilities[is.na(house$Utilities)] <- "AllPub"
house$Exterior1st[is.na(house$Exterior1st)] <- "VinylSd"
house$Exterior2nd[is.na(house$Exterior2nd)] <- "VinylSd"
house$Electrical[is.na(house$Electrical)] <- "SBrkr"
house$KitchenQual[is.na(house$KitchenQual)] <- "TA"
house$Functional[is.na(house$Functional)] <- "Typ"
house$SaleType[is.na(house$SaleType)] <- "WD"

for (i in 1:nrow(house)) {
  ## Check logic on Masonry veneer
  if (is.na(house$MasVnrArea[i])) {
    if (house$MasVnrType[i] == "None") {
      house$MasVnrArea[i] <- 0  # if there isn't any masonry veneer, then the NA should be replaced with 0
    } else {
      house$MasVnrArea[i] <- mean(house$MasVnrArea, na.rm=T)  # if there is veneer, replace with the average
    }
  }
  
  ## Basement
  if (is.na(house$BsmtFinSF1[i])) {
    if (house$BsmtQual[i] != "N/A") {
      # if there is a basement, then use the average
      house$BsmtFinSF1[i] <- mean(house$BsmtFinSF1, na.rm=T)
    } else {
      # if there isn't a basement, use 0
      house$BsmtFinSF1[i] <- 0
    }
  }
  if (is.na(house$BsmtFinSF2[i])) {
    if (house$BsmtQual[i]!="N/A") {
      house$BsmtFinSF2[i] <- mean(house$BsmtFinSF2, na.rm=T)
    } else {
      house$BsmtFinSF2[i] <- 0
    }
  }
  if (is.na(house$BsmtUnfSF[i])) {
    if (house$BsmtQual[i]!="N/A") {
      house$BsmtUnfSF[i] <- mean(house$BsmtUnfSF, na.rm=T)
    } else {
      house$BsmtUnfSF[i] <- 0
    }
  }
  if (is.na(house$TotalBsmtSF[i])) {
    if (house$BsmtQual[i]!="N/A") {
      house$TotalBsmtSF[i] <- mean(house$TotalBsmtSF, na.rm=T)
    } else {
      house$TotalBsmtSF[i] <- 0
    }
  }
  if (is.na(house$BsmtFullBath[i])) {
    if (house$BsmtQual[i]!="N/A") {
      house$BsmtFullBath[i] <- mean(house$BsmtFullBath, na.rm=T)
    } else {
      house$BsmtFullBath[i] <- 0
    }
  }
  if (is.na(house$BsmtHalfBath[i])) {
    if (house$BsmtQual[i]!="N/A") {
      house$BsmtHalfBath[i] <- mean(house$BsmtHalfBath, na.rm=T)
    } else {
      house$BsmtHalfBath[i] <- 0
    }
  }

  ## Garage
  if (is.na(house$GarageCars[i])) {
    if (house$GarageType[i] != "N/A") {
      # if there is a garage, then use the average
      house$GarageCars[i] <- mean(house$GarageCars, na.rm=T)
    } else {
      # if there isn't a basement, use 0
      house$GarageCars[i] <- 0
    }
  }
  if (is.na(house$GarageArea[i])) {
    if (house$GarageType[i] != "N/A") {
      # if there is a garage, then use the average
      house$GarageArea[i] <- mean(house$GarageArea, na.rm=T)
    } else {
      # if there isn't a basement, use 0
      house$GarageArea[i] <- 0
    }
  }
}


## Note which columns need to have NAs interpolated
ct_na <- rep(0, length=ncol(house))
for (j in 1:ncol(house)) {
  ct_na[j] <- sum(is.na(house[,j]))
}
ct_na
na_boo <- ifelse(ct_na!=0, T, F)
colnames(house)[na_boo]  # all set!
```


### Split
The following code splits the data back into the training and testing sets.
```{r}
train <- house[1:1460,]
test <- house[1461:2919,]
```






### Linear Diagnostics
The following section will look at linear diagnostics to evaluate any other data changes that need to be made.

```{r}
lin_fit <- lm(SalePrice~.-Id, data=train)
plot(lin_fit)
```

Based on the warning message, points 121, 186, 250, 325, 332, 346, 375, 398, 532, 582, 594, 664, 808, 819, 941, 945, 998, 1006, 1182, 1225, 1264, 1269, 1291, 1314, 1363, 1378 should be removed. Points 1171 and 1424 also have high leverage, so they will be removed. The residuals vs. fitted plot  looks pretty good (so the data is reasonably linear and tere is a fairly constant variance of the error terms). The outliers (826, 524) are removed because they are also high leverage points.

```{r}
train <- train[-c(121,186,250,325,332,346,375,398,524,532,582,594,664,808,819,826,941,945,998,1006,1171,1182,1225,1264,1269,1291,1314,1363,1378,1424),]
```

After removing the statistically "bad" points, the variable `Utilities` can be removed because they all have the same value
```{r}
train <- train[,-9]; test <- test[,-9]
```

It was discussed during the midterm that models may fit better when the response is log-transformed.



Here, I set a seed for reproducibility.
```{r}
set.seed(1)
```



## Decision Trees
In this section, I will explore a very large tree and then prune the tree down. I will try this using both the response as-is and the log-transformed response.

Decision trees require the `tree` library.
```{r}
library(tree)
```


### Big Tree
```{r}
tree.log.model <- tree(log(SalePrice)~.-Id, data=train)
print(tree.log.model); print(summary(tree.log.model))
```
The "big" tree fit to the log-transformed response has 11 terminal nodes using `OverallQual`, `Neighborhood`, `GrLivArea`, and `CentralAir` as key predictors.

The following code creates the plot.
```{r}
plot(tree.log.model, type="uniform")
text(tree.log.model, pretty=0, cex=0.6)
```

The following code creates the prediction using the big-tree model.
```{r}
tree.log.prediction <- exp(predict(tree.log.model, newdata=test))  # exp to return to actual price instead of log price

write.csv(data.frame(Id=test$Id, SalePrice=tree.log.prediction), "Tree_Log_Prediction.csv", row.names=F)
```

The Kaggle score for this model was **0.22102**.

The following code uses cross-validation to predict the error.
```{r}
set.seed(428) # consistency of k-fold validation breaks
fold.index <- cut(sample(1:nrow(train)), breaks=10, labels=FALSE) # split data into 10 folds

out <- c()
for (i in 1:10) {
  train.cv <- train[fold.index!=i,]
  test.cv.X <- train[fold.index==i,-77]
  test.cv.y <- train[fold.index==i,77]

  tree.log.model.cv <- tree(log(SalePrice)~.-Id, data=train.cv)
  tree.log.prediction.cv <- exp(predict(tree.log.model.cv, newdata=test.cv.X))
  tree.log.rmsle.cv <- sqrt(mean((log(tree.log.prediction.cv)-log(test.cv.y))^2))
  out <- c(out, tree.log.rmsle.cv)
}
mean(out)
```
The estimated RMSLE is 0.2091555 for the (unpruned) decision tree method; this is quite close to the true test RMSLE.



### Pruned Tree
The following code performs cross-validation to determine the optimal tree size, then prunes the tree to that size.
```{r}
set.seed(428)  # set seed for consistency
cv.tree.log.model <- cv.tree(tree.log.model, FUN=prune.tree, K=10)
tree.prune.log.size <- cv.tree.log.model$size[which.min(cv.tree.log.model$dev)]
tree.prune.log.model <- prune.tree(tree.log.model, best=tree.prune.log.size)
```

The following code describes the pruned tree.
```{r}
tree.prune.log.model
plot(tree.prune.log.model, type="uniform"); text(tree.prune.log.model, pretty=0, cex=0.6)
summary(tree.prune.log.model)
```
The pruned tree has 10 nodes and uses the predictors `OverallQual`, `Neighborhood`, and `GrLivArea`; this pruned tree no longer uses `CentralAir` as a predictor.

The following code creates the prediction using the pruned-tree model.
```{r}
tree.prune.log.prediction <- exp(predict(tree.prune.log.model, newdata=test))
write.csv(data.frame(Id=test$Id, SalePrice=tree.prune.log.prediction), "Tree_Prune_Log_Prediction.csv", row.names=F)
```

The Kaggle score for this model was **0.22102**. This method performed the same as the big decision tree, which had one more terminal node than the pruned tree.

The following code uses cross-validation to predict the error.
```{r}
set.seed(428) # consistency of k-fold validation breaks
fold.index <- cut(sample(1:nrow(train)), breaks=10, labels=FALSE) # split data into 10 folds

out <- c()
for (i in 1:10) {
  train.cv <- train[fold.index!=i,]
  test.cv.X <- train[fold.index==i,-77]
  test.cv.y <- train[fold.index==i,77]

  tree.log.model.cv <- tree(log(SalePrice)~.-Id, data=train.cv)
  tree.log.model.cv.cv <- cv.tree(tree.log.model.cv, FUN=prune.tree, K=10)
  tree.log.model.size.cv <- tree.log.model.cv.cv$size[which.min(tree.log.model.cv.cv$dev)]
  tree.prune.log.model.cv <- prune.tree(tree.log.model.cv, best=tree.log.model.size.cv)
  tree.prune.log.prediction.cv <- exp(predict(tree.prune.log.model.cv, newdata=test.cv.X))
  tree.prune.log.rmsle.cv <- sqrt(mean((log(tree.prune.log.prediction.cv)-log(test.cv.y))^2))
  out <- c(out, tree.prune.log.rmsle.cv)
}
mean(out)
```
The estimated RMSLE is 0.2091555 for the pruned decision tree method; this is fairly close to the true test RMSLE.


### Big Tree
```{r}
tree.model <- tree(SalePrice~.-Id, data=train)
print(tree.model); print(summary(tree.model))
```
The "big" tree fit to the log-transformed response has 12 terminal nodes using `OverallQual`, `Neighborhood`, `1stFlrSF`, `GrLivArea`, `BsmtFinSf1`, `MoSold`, and `MasVnrArea` as key predictors. This includes more and different predictors than the log-transformed response, but in this case `CentralAir` was left out as a predictor.

The following code creates the plot.
```{r}
plot(tree.model, type="uniform")
text(tree.model, pretty=0, cex=0.6)
```

The following code creates the prediction using the big-tree model.
```{r}
tree.prediction <- predict(tree.model, newdata=test)
write.csv(data.frame(Id=test$Id, SalePrice=tree.prediction), "Tree_Prediction.csv", row.names=F)
```

The Kaggle score for this model was **0.24409**. This big tree performed worse than the big tree that had a log-transformed response.

The following code uses cross-validation to predict the error.
```{r}
set.seed(428) # consistency of k-fold validation breaks
fold.index <- cut(sample(1:nrow(train)), breaks=10, labels=FALSE) # split data into 10 folds

out <- c()
for (i in 1:10) {
  train.cv <- train[fold.index!=i,]
  test.cv.X <- train[fold.index==i,-77]
  test.cv.y <- train[fold.index==i,77]

  tree.model.cv <- tree(SalePrice~.-Id, data=train.cv)
  tree.prediction.cv <- predict(tree.model.cv, newdata=test.cv.X)
  tree.rmsle.cv <- sqrt(mean((log(tree.prediction.cv)-log(test.cv.y))^2))
  out <- c(out, tree.rmsle.cv)
}
mean(out)
```
The estimated RMSLE is 0.2245694 for the (unpruned) decision tree method; this is fairly close to the true test RMSLE.



### Pruned Tree
The following code performs cross-validation to determine the optimal tree size, then prunes the tree to that size.
```{r}
set.seed(428)  # set seed for consistency
cv.tree.model <- cv.tree(tree.model, FUN=prune.tree, K=10)
tree.prune.size <- cv.tree.model$size[which.min(cv.tree.model$dev)]
tree.prune.model <- prune.tree(tree.model, best=tree.prune.size)
```

The following code describes the pruned tree.
```{r}
tree.prune.model
plot(tree.prune.model, type="uniform"); text(tree.prune.model, pretty=0, cex=0.6)
summary(tree.prune.model)
```
The pruned tree has 12 nodes and uses the predictors `OverallQual`, `Neighborhood`, `1stFlrSF`, `GrLivArea`, `BsmtFinSF1`, and `MoSold`. This tree is the same as the unpruned model, so there is no need to go farther.



## Bagging
```{r}
library(randomForest)

bag.mtry <- ncol(train)-2  # split candidate count; don't include response or ID
```

This next section, like the previous, will try to fit the bagging model to both the log-transformed response and the response as-is. The maximum number of trees that provided good computational time was `ntree=500`.
```{r}
bag.log.model <- randomForest(log(SalePrice)~.-Id, data=train, mtry=bag.mtry, importance=T, ntree=500)
```

The results of the bagged model on the log-transformed response are as follows:
```{r}
varImpPlot(bag.log.model)
sort(importance(bag.log.model)[,1], decreasing=T)
sort(importance(bag.log.model)[,2], decreasing=T)
```
In terms of both prediction accuracy `[,1]` and purity `[,2]`, the three most important variables are `OverallQual`, `GrLivArea`, and `Neighborhood`; this is consistent with the results from the decision trees. `TotalBsmtSF` is also a somewhat important predictor (more in terms of prediction accuracy than purity), which is consistent with the results for the non-log-transformed response decision tree model.

The following code creates a prediction using the bagging model.
```{r}
bag.log.prediction <- exp(predict(bag.log.model, newdata=test))
write.csv(data.frame(Id=test$Id, SalePrice=bag.log.prediction), "Bagging_Log_Prediction.csv", row.names=F)
```

The Kaggle score for this model was **0.15489**. This shows significant improvement over the decision tree method.

The following code uses cross-validation to predict the error.
```{r}
set.seed(428) # consistency of k-fold validation breaks
fold.index <- cut(sample(1:nrow(train)), breaks=10, labels=FALSE) # split data into 10 folds

out <- c()
for (i in 1:10) {
  train.cv <- train[fold.index!=i,]
  test.cv.X <- train[fold.index==i,-77]
  test.cv.y <- train[fold.index==i,77]

  bag.log.model.cv <- randomForest(log(SalePrice)~.-Id, data=train.cv, mtry=bag.mtry, importance=T, ntree=500)
  bag.log.prediction.cv <- exp(predict(bag.log.model.cv, newdata=test.cv.X))
  bag.log.rmsle.cv <- sqrt(mean((log(bag.log.prediction.cv)-log(test.cv.y))^2))
  out <- c(out, bag.log.rmsle.cv)
}
mean(out)
```
The estimated RMSLE is 0.1427881 for the bagging method; this is fairly close to the true test RMSLE.


The following code fits a bagging model to the data, but doesn't log-transform the response.
```{r}
bag.model <- randomForest(SalePrice~.-Id, data=train, mtry=bag.mtry, importance=T, ntree=500)
```

The results of the bagged model on the non-transformed response are as follows:
```{r}
varImpPlot(bag.model)
sort(importance(bag.model)[,1], decreasing=T)
sort(importance(bag.model)[,2], decreasing=T)
```
In terms of both prediction accuracy `[,1]` and purity `[,2]`, the two three most important variables are `OverallQual` and `Neighborhood`; this is consistent with the results from the decision trees. In terms of prediction accuracy, the variable `GrLivArea` is the number one predictor, but it is ranked third for node purity.

The following code creates a prediction using the bagging model.
```{r}
bag.prediction <- predict(bag.model, newdata=test)
write.csv(data.frame(Id=test$Id, SalePrice=bag.prediction), "Bagging_Prediction.csv", row.names=F)
```

The Kaggle score for this model was **0.15402**. This shows significant improvement over the decision tree method and is even better than the log-transformed bagging model.

The following code uses cross-validation to predict the error.
```{r}
set.seed(428) # consistency of k-fold validation breaks
fold.index <- cut(sample(1:nrow(train)), breaks=10, labels=FALSE) # split data into 10 folds

out <- c()
for (i in 1:10) {
  train.cv <- train[fold.index!=i,]
  test.cv.X <- train[fold.index==i,-77]
  test.cv.y <- train[fold.index==i,77]

  bag.model.cv <- randomForest(SalePrice~.-Id, data=train.cv, mtry=bag.mtry, importance=T, ntree=500)
  bag.prediction.cv <- predict(bag.model.cv, newdata=test.cv.X)
  bag.rmsle.cv <- sqrt(mean((log(bag.prediction.cv)-log(test.cv.y))^2))
  out <- c(out, bag.rmsle.cv)
}
mean(out)
```
The estimated RMSLE is 0.1463885 for the bagging method on the non-transformed response; this is fairly close to the true test RMSLE.



### Random Forest
```{r}
library(randomForest)

rf.mtry <- round(sqrt(ncol(train)-2))  # split candidate count; don't include response or ID
```

This next section, like the previous, will try to fit the random forest model to both the log-transformed response and the response as-is. The maximum number of trees that provided good computational time was `ntree=500`.
```{r}
rf.log.model <- randomForest(log(SalePrice)~.-Id, data=train, mtry=rf.mtry, importance=T, ntree=500)
```

The results of the random forest model on the log-transformed response are as follows:
```{r}
varImpPlot(rf.log.model)
sort(importance(rf.log.model)[,1], decreasing=T)
sort(importance(rf.log.model)[,2], decreasing=T)
```
In terms of both prediction accuracy `[,1]` and purity `[,2]`, the three most important variables include `GrLivArea`, and `Neighborhood`; this is consistent with the results from the decision trees and the bagging. `GrLivArea` is an important predictor of accuracy, but `OverallQual` is more important for node purity.

The following code creates a prediction using the random forest model.
```{r}
rf.log.prediction <- exp(predict(rf.log.model, newdata=test))
write.csv(data.frame(Id=test$Id, SalePrice=rf.log.prediction), "RandomForest_Log_Prediction.csv", row.names=F)
```

The Kaggle score for this model was **0.15158**. This shows significant improvement over the decision tree method. This shows improvement over the bagging-log model.

The following code uses cross-validation to predict the error.
```{r}
set.seed(428) # consistency of k-fold validation breaks
fold.index <- cut(sample(1:nrow(train)), breaks=10, labels=FALSE) # split data into 10 folds

out <- c()
for (i in 1:10) {
  train.cv <- train[fold.index!=i,]
  test.cv.X <- train[fold.index==i,-77]
  test.cv.y <- train[fold.index==i,77]

  rf.log.model.cv <- randomForest(log(SalePrice)~.-Id, data=train.cv, mtry=rf.mtry, importance=T, ntree=500)
  rf.log.prediction.cv <- exp(predict(rf.log.model.cv, newdata=test.cv.X))
  rf.log.rmsle.cv <- sqrt(mean((log(rf.log.prediction.cv)-log(test.cv.y))^2))
  out <- c(out, rf.log.rmsle.cv)
}
mean(out)
```
The estimated RMSLE is 0.1401211 for the random forest method; this is fairly close to the true test RMSLE.


The following code fits a random forest model to the data, but doesn't log-transform the response.
```{r}
rf.model <- randomForest(SalePrice~.-Id, data=train, mtry=rf.mtry, importance=T, ntree=500)
```

The results of the random forest model on the non-transformed response are as follows:
```{r}
varImpPlot(rf.model)
sort(importance(rf.model)[,1], decreasing=T)
sort(importance(rf.model)[,2], decreasing=T)
```
In terms of both prediction accuracy `[,1]` and purity `[,2]`, the two three most important variables are `GrLivArea`, `OverallQual` and `Neighborhood` (though not in the same overder for both); this is consistent with the results from the decision trees and bagging. In terms of prediction accuracy, the variable `GrLivArea` is the number one predictor, but it is ranked third for node purity.

The following code creates a prediction using the random forest model.
```{r}
rf.prediction <- predict(rf.model, newdata=test)
write.csv(data.frame(Id=test$Id, SalePrice=rf.prediction), "RandomForest_Prediction.csv", row.names=F)
```

The Kaggle score for this model was **0.15819**. This shows significant improvement over the decision tree method, but not the non-transformed bagging model, and it is worse than the log-transformed response random forest model.

The following code uses cross-validation to predict the error.
```{r}
set.seed(428) # consistency of k-fold validation breaks
fold.index <- cut(sample(1:nrow(train)), breaks=10, labels=FALSE) # split data into 10 folds

out <- c()
for (i in 1:10) {
  train.cv <- train[fold.index!=i,]
  test.cv.X <- train[fold.index==i,-77]
  test.cv.y <- train[fold.index==i,77]

  rf.model.cv <- randomForest(SalePrice~.-Id, data=train.cv, mtry=rf.mtry, importance=T, ntree=500)
  rf.prediction.cv <- predict(rf.model.cv, newdata=test.cv.X)
  rf.rmsle.cv <- sqrt(mean((log(rf.prediction.cv)-log(test.cv.y))^2))
  out <- c(out, rf.rmsle.cv)
}
mean(out)
```
The estimated RMSLE is 0.1445637 for the random forest method on the non-transformed response; this is fairly close to the true test RMSLE.



## Boosting
This next section, like the previous, will try to fit the boosting model to both the log-transformed response and the response as-is. It will also choose the parameters (number of trees, interaction depth, and shrinkage) using cross-validation.

```{r}
library(gbm)
```

Using cross-validation, I will check what combination of $B$, $\lambda$, and $d$ results in the smallest estimated test error. I use common values for $\lambda$ and $d$, as discussed in class.
```{r}
boost.trees.params <- seq(25, 250, 25)
boost.lambda.params <- c(0.1, 0.01)
boost.depth.params <- c(1, 2, 4, 8)

boost.log.cv.rmsle.array <- array(NA, dim=c(length(boost.trees.params), length(boost.lambda.params), length(boost.depth.params)), dimnames=list(boost.trees.params, boost.lambda.params, boost.depth.params))

set.seed(428) # consistency of k-fold validation breaks
fold.index <- cut(sample(1:nrow(train)), breaks=10, labels=FALSE) # split data into 10 folds

for (i in 1:length(boost.trees.params)) {
  for (j in 1:length(boost.lambda.params)) {
    for (k in 1:length(boost.depth.params)) {
      
      out <- c()
      for (l in 1:10) {
  
        train.cv <- train[fold.index!=l,]
        test.cv.X <- train[fold.index==l,-77]
        test.cv.y <- train[fold.index==l,77]
        
        tmp.log.model <- gbm(log(SalePrice)~.-Id, data=train.cv, distribution='gaussian', n.trees=boost.trees.params[i], interaction.depth=boost.depth.params[k], shrinkage=boost.lambda.params[j])
        tmp.prediction <- exp(predict(tmp.log.model, newdata=test.cv.X, n.trees=boost.trees.params[i]))
        out[l] <- mean((log(tmp.prediction)-log(test.cv.y))^2)
        
#        print(paste0("i=",i,",j=",j,",k=",k,",l=",l))
      }
      boost.log.cv.rmsle.array[i,j,k] <- mean(out)
    }
  }
}

which(boost.log.cv.rmsle.array==min(boost.log.cv.rmsle.array), arr.ind=T)
```
The best is 1,1,4 which is 25 trees, 0.1 shrinkage, and depth of 8. (est. RMSLE is 0.06577063).

The following code creates using these parameter values.
```{r}
boost.log.model <- gbm(log(SalePrice)~.-Id, data=train, distribution='gaussian', n.trees=25, interaction.depth=8, shrinkage=0.1)
```

The following code looks at the important variables in the boosting model.
```{r}
summary(boost.log.model)
```

The three most important variables in this model are `OverallQual`, `GrLivArea`, and `Neighborhood`; this is consistent with the other models.

The following code makes the prediction using the model.
```{r}
boost.log.prediction <- exp(predict(boost.log.model, newdata=test, n.trees=25))
write.csv(data.frame(Id=test$Id, SalePrice=boost.log.prediction), "Boosting_Log_Prediction.csv", row.names=F)
```

The Kaggle score for this model was **0.15386**. This shows improvement over the decision trees and the bagging model for the log-transformed response; it was slightly worse than the random forest model.


Using cross-validation, I will check what combination of $B$, $\lambda$, and $d$ results in the smallest estimated test error. I use common values for $\lambda$ and $d$, as discussed in class.
```{r}
boost.trees.params <- seq(25, 250, 25)
boost.lambda.params <- c(0.1, 0.01)
boost.depth.params <- c(1, 2, 4, 8)

boost.cv.rmsle.array <- array(NA, dim=c(length(boost.trees.params), length(boost.lambda.params), length(boost.depth.params)), dimnames=list(boost.trees.params, boost.lambda.params, boost.depth.params))

set.seed(428) # consistency of k-fold validation breaks
fold.index <- cut(sample(1:nrow(train)), breaks=10, labels=FALSE) # split data into 10 folds

for (i in 1:length(boost.trees.params)) {
  for (j in 1:length(boost.lambda.params)) {
    for (k in 1:length(boost.depth.params)) {
      
      out <- c()
      for (l in 1:10) {
  
        train.cv <- train[fold.index!=l,]
        test.cv.X <- train[fold.index==l,-77]
        test.cv.y <- train[fold.index==l,77]
        
        tmp.model <- gbm(SalePrice~.-Id, data=train.cv, distribution='gaussian', n.trees=boost.trees.params[i], interaction.depth=boost.depth.params[k], shrinkage=boost.lambda.params[j])
        tmp.prediction <- predict(tmp.model, newdata=test.cv.X, n.trees=boost.trees.params[i])
        out[l] <- mean((log(tmp.prediction)-log(test.cv.y))^2)
        
       # print(paste0("i=",i,",j=",j,",k=",k,",l=",l))
      }
      boost.cv.rmsle.array[i,j,k] <- mean(out)
    }
  }
}

which(boost.cv.rmsle.array==min(boost.cv.rmsle.array), arr.ind=T)
min(boost.cv.rmsle.array)
```

The best is 10,2,4 which is 250 trees, 0.01 shrinkage, and depth of 8. (est. RMSLE is 0.06348275).

The following code creates using these parameter values.
```{r}
boost.model <- gbm(SalePrice~.-Id, data=train, distribution='gaussian', n.trees=250, interaction.depth=8, shrinkage=0.01)
```

The following code looks at the important variables in the boosting model.
```{r}
summary(boost.model)
```

The three most important variables in this model are `OverallQual`, `GrLivArea`, and `Neighborhood`; this is consistent with the other models.

The following code makes the prediction using the model.
```{r}
boost.prediction <- predict(boost.model, newdata=test, n.trees=250)
write.csv(data.frame(Id=test$Id, SalePrice=boost.prediction), "Boosting_Prediction.csv", row.names=F)
```

The Kaggle score for this model was **0.17266**. This shows improvement over the decision tree, but not over anything else.
